{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297be273",
   "metadata": {},
   "source": [
    "## ID: 2446553\n",
    "## Module code: [06-37812]\n",
    "\n",
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be1c22",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be82874",
   "metadata": {
    "id": "0be82874"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 02:52:59.736195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from textblob import TextBlob \n",
    "import spacy\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e460e",
   "metadata": {
    "id": "034e460e"
   },
   "source": [
    "## XML Parsing\n",
    "\n",
    "Here, I use the beautiful_soup package to parse the xml file and extract the sentences and opinions. Once I am done, I make a sentence-opinion pair for each sentence and opinion. Then, I save it all in a dictionary which I then convert into a pandas dataframe for easy processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32352fb3",
   "metadata": {
    "id": "32352fb3"
   },
   "outputs": [],
   "source": [
    "def parse_xml(filename):\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    data = ''.join(data)\n",
    "    content = bs(data, 'xml')\n",
    "    data = {\n",
    "        'text': [],\n",
    "        'entity': [],\n",
    "        'attribute': [],\n",
    "        'polarity': []\n",
    "    }\n",
    "\n",
    "    rid_list = []\n",
    "    a = list(content.find_all('Review'))\n",
    "    for i in a:\n",
    "        rid_list.append(i.get('rid'))\n",
    "        \n",
    "    for rid in rid_list:\n",
    "        for sentence in content.find('Review', {'rid': rid}).find_all('sentence'):\n",
    "            if sentence.find('Opinions'):\n",
    "                for opinion in sentence.find('Opinions').find_all('Opinion'):\n",
    "                    data['text'].append(sentence.find('text').text)\n",
    "                    category = opinion.get('category')\n",
    "                    entity, attribute = category.split('#')\n",
    "                    data['entity'].append(entity)\n",
    "                    data['attribute'].append(attribute)\n",
    "                    data['polarity'].append(opinion.get('polarity'))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8f699",
   "metadata": {
    "id": "65e8f699"
   },
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "As for preprocessing, I go for the usual tokenization, stop word removal, and then lemmatization. I spotted some misspellings, and tried using the TextBlob package to correct them, but it honestly caused a lot more trouble than good. It converted a lot of correctly spelt words into random combinations of letters. It also took a significant amount of processing time anyway, so it was removed. \n",
    "\n",
    "I also remove quite a few words from the stopword list, as they were quite important to our task. For example, the word 'not' is important to differentiate between \"happy\" and \"not happy\". Similar argument for the rest of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1173faba",
   "metadata": {
    "id": "1173faba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Tokenization \n",
    "def process_text(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-.']\\w+)*\")\n",
    "    sw_list = stopwords.words('english')\n",
    "    remove_words = ['not', 'used', 'never', \"don't\", 'care', \"didn't\", 'cannot', \"didn'\", \"hasn't\", \"haven't\", \"isn\", \"isn't\", \"mightn't\", \"mustn't\", \"might\",\"needn't\",\"shan't\",\"shouldn't\", \"wasn't\", \"weren't\", \"doesn't\", \"won't\", \"wouldn't\"]\n",
    "    add_words = ['homework', 'student', 'science', 'college']\n",
    "    sw_list = [word for word in sw_list if not word in remove_words]\n",
    "    sw_list.extend(add_words)\n",
    "\n",
    "    lemmatized_sent = []\n",
    "\n",
    "    for i in tqdm(range(len(df.text))):\n",
    "\n",
    "        text = df.text[i]\n",
    "        text = str.lower(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "#         for i in range(len(tokens)):\n",
    "#             tokens[i] = str(TextBlob(tokens[i]).correct())\n",
    "        tokens = [word for word in tokens if not word in sw_list]\n",
    "        lemmatized_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "        lemmatized_sent.append(' '.join(lemmatized_words))\n",
    "    \n",
    "    df['processed'] = lemmatized_sent\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa628132",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa628132",
    "outputId": "3a5b6946-102e-471d-adf9-ea4de6450871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2909/2909 [00:01<00:00, 2117.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df = parse_xml('/content/Laptops_Train_p1.xml') ## Please give the approp file path here. \n",
    "orig = process_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94293c",
   "metadata": {
    "id": "9e94293c"
   },
   "source": [
    "## Aspect Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d5d671",
   "metadata": {
    "id": "08d5d671"
   },
   "outputs": [],
   "source": [
    "entity_labels = ['laptop', 'display', 'keyboard', 'mouse', 'motherboard', 'cpu', 'fans_cooling', \n",
    "                 'ports', 'memory', 'power_supply', 'optical_drives', 'battery', 'graphics', \n",
    "                 'hard_disk', 'multimedia_devices', 'hardware', 'software', 'os', 'warranty', 'shipping', 'support', 'company']\n",
    "\n",
    "attribute_labels = ['general', 'price', 'quality', 'design_feature', 'operation_performance', 'usability', 'portability',\n",
    "                   'connectivity', 'miscellaneous']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb0e53",
   "metadata": {},
   "source": [
    "Here, we perform chunking using the spacy package to extract noun chunks to form the aspect terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e28153",
   "metadata": {
    "id": "08e28153"
   },
   "outputs": [],
   "source": [
    "df = orig.copy()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "aspect_terms = []\n",
    "for i, review in enumerate(nlp.pipe(df.processed)):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    if not len(chunks):\n",
    "        df.drop(i, inplace=True)\n",
    "        continue\n",
    "    aspect_terms.append(' '.join(chunks))\n",
    "\n",
    "df['aspect terms'] = aspect_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9038af",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "I go for separate models to predict the entity, the aspect and the sentiment. I have seen quite a few examples out there that try to predict the aspect#entity term with a single network and not separating them, but I saw that the dataset had 81 such unique pairs alone, let alone all combinations. I felt that this would make for an inefficient model, and went for this type of prediction. \n",
    "\n",
    "For all models, I go for a simple and standard feedforward network as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d29f0d",
   "metadata": {
    "id": "48d29f0d"
   },
   "outputs": [],
   "source": [
    "entity_categories_model = Sequential()\n",
    "entity_categories_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "entity_categories_model.add(Dense(512, activation='relu'))\n",
    "entity_categories_model.add(Dense(128, activation='relu'))\n",
    "entity_categories_model.add(Dense(len(entity_labels), activation='softmax'))\n",
    "entity_categories_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b27ee",
   "metadata": {},
   "source": [
    "Here, I use Keras' Tokenizer to vectorize the text. (Ref: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
    "I then use a label encoder and then convert the labels into one-hot encoded format as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229b5088",
   "metadata": {
    "id": "229b5088"
   },
   "outputs": [],
   "source": [
    "vocab_size = 6000 \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(df['aspect terms']))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_entity = label_encoder.fit_transform(df.entity)\n",
    "entity_category = to_categorical(integer_entity)\n",
    "integer_attribute = label_encoder.fit_transform(df.attribute)\n",
    "attribute_category = to_categorical(integer_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b13438",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6b13438",
    "outputId": "48bbe7cd-d888-4425-dfd3-b3b8cb1d4ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "78/78 [==============================] - 6s 5ms/step - loss: 1.7308 - accuracy: 0.6340\n",
      "Epoch 2/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.2913 - accuracy: 0.6522\n",
      "Epoch 3/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0465 - accuracy: 0.6943\n",
      "Epoch 4/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8762 - accuracy: 0.7421\n",
      "Epoch 5/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.7865 - accuracy: 0.7466\n",
      "Epoch 6/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.7267 - accuracy: 0.7644\n",
      "Epoch 7/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.7619\n",
      "Epoch 8/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6489 - accuracy: 0.7717\n",
      "Epoch 9/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6339 - accuracy: 0.7700\n",
      "Epoch 10/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6179 - accuracy: 0.7785\n",
      "Epoch 11/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6095 - accuracy: 0.7717\n",
      "Epoch 12/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5886 - accuracy: 0.7822\n",
      "Epoch 13/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5834 - accuracy: 0.7826\n",
      "Epoch 14/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5778 - accuracy: 0.7725\n",
      "Epoch 15/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7862\n",
      "Epoch 16/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5549 - accuracy: 0.7826\n",
      "Epoch 17/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5452 - accuracy: 0.7854\n",
      "Epoch 18/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5485 - accuracy: 0.7777\n",
      "Epoch 19/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5344 - accuracy: 0.7850\n",
      "Epoch 20/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5300 - accuracy: 0.7818\n",
      "Epoch 21/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5363 - accuracy: 0.7745\n",
      "Epoch 22/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5228 - accuracy: 0.7891\n",
      "Epoch 23/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5213 - accuracy: 0.7899\n",
      "Epoch 24/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5223 - accuracy: 0.7862\n",
      "Epoch 25/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5159 - accuracy: 0.7822\n",
      "Epoch 26/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5195 - accuracy: 0.7818\n",
      "Epoch 27/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5121 - accuracy: 0.7874\n",
      "Epoch 28/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5178 - accuracy: 0.7733\n",
      "Epoch 29/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5103 - accuracy: 0.7842\n",
      "Epoch 30/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5116 - accuracy: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbf8cd1d00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_categories_model.fit(aspect_tokenized, entity_category, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f6b828",
   "metadata": {
    "id": "00f6b828"
   },
   "outputs": [],
   "source": [
    "attribute_categories_model = Sequential()\n",
    "attribute_categories_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "attribute_categories_model.add(Dense(512, activation='relu'))\n",
    "attribute_categories_model.add(Dense(128, activation='relu'))\n",
    "attribute_categories_model.add(Dense(len(attribute_labels), activation='softmax'))\n",
    "attribute_categories_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6ad7d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa6ad7d7",
    "outputId": "e6397520-ffd4-49c3-f63d-13e9f58571ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "78/78 [==============================] - 2s 5ms/step - loss: 1.8993 - accuracy: 0.3020\n",
      "Epoch 2/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.5913 - accuracy: 0.4417\n",
      "Epoch 3/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.3697 - accuracy: 0.5206\n",
      "Epoch 4/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.2545 - accuracy: 0.5332\n",
      "Epoch 5/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.1770 - accuracy: 0.5510\n",
      "Epoch 6/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.1300 - accuracy: 0.5579\n",
      "Epoch 7/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0848 - accuracy: 0.5563\n",
      "Epoch 8/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0575 - accuracy: 0.5628\n",
      "Epoch 9/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0326 - accuracy: 0.5717\n",
      "Epoch 10/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0053 - accuracy: 0.5729\n",
      "Epoch 11/30\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.9928 - accuracy: 0.5733\n",
      "Epoch 12/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9800 - accuracy: 0.5676\n",
      "Epoch 13/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9608 - accuracy: 0.5704\n",
      "Epoch 14/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9582 - accuracy: 0.5684\n",
      "Epoch 15/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.9439 - accuracy: 0.5579\n",
      "Epoch 16/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.9436 - accuracy: 0.5729\n",
      "Epoch 17/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.9281 - accuracy: 0.5802\n",
      "Epoch 18/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.9193 - accuracy: 0.5676\n",
      "Epoch 19/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9165 - accuracy: 0.5806\n",
      "Epoch 20/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9113 - accuracy: 0.5806\n",
      "Epoch 21/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9083 - accuracy: 0.5789\n",
      "Epoch 22/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9050 - accuracy: 0.5822\n",
      "Epoch 23/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8975 - accuracy: 0.5789\n",
      "Epoch 24/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8910 - accuracy: 0.5895\n",
      "Epoch 25/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8859 - accuracy: 0.5854\n",
      "Epoch 26/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8897 - accuracy: 0.5806\n",
      "Epoch 27/30\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.8885 - accuracy: 0.5830\n",
      "Epoch 28/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8881 - accuracy: 0.5850\n",
      "Epoch 29/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8774 - accuracy: 0.5846\n",
      "Epoch 30/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8714 - accuracy: 0.5883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbf20bdb20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_categories_model.fit(aspect_tokenized, attribute_category, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25826a57",
   "metadata": {},
   "source": [
    "## Sentiment extraction\n",
    "\n",
    "I do the same for the sentiment terms, but extracting the adjectives and verbs from the sentences this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb41787",
   "metadata": {
    "id": "dcb41787"
   },
   "outputs": [],
   "source": [
    "df = orig.copy()\n",
    "sentiment_terms = []\n",
    "rem_ind = []\n",
    "for i, review in enumerate(nlp.pipe(df['processed'])):\n",
    "    chunks = [token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]\n",
    "    if not len(chunks):\n",
    "        rem_ind.append(i)\n",
    "        continue\n",
    "    sentiment_terms.append(' '.join(chunks))\n",
    "df = df.drop(rem_ind)\n",
    "df['sentiment_terms'] = sentiment_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2100836a",
   "metadata": {
    "id": "2100836a"
   },
   "outputs": [],
   "source": [
    "sentiment_model = Sequential()\n",
    "sentiment_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "sentiment_model.add(Dense(512, activation='relu'))\n",
    "sentiment_model.add(Dense(256, activation='relu'))\n",
    "sentiment_model.add(Dense(128, activation='relu'))\n",
    "sentiment_model.add(Dense(3, activation='softmax'))\n",
    "sentiment_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3602e335",
   "metadata": {
    "id": "3602e335"
   },
   "outputs": [],
   "source": [
    "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(df.sentiment_terms))\n",
    "label_encoder = LabelEncoder()\n",
    "integer_sentiment = label_encoder.fit_transform(df.polarity)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8a20351",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8a20351",
    "outputId": "24fa9d9b-732f-44b3-833b-cfaf7bae2cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "86/86 [==============================] - 2s 5ms/step - loss: 0.7512 - accuracy: 0.6784\n",
      "Epoch 2/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5311 - accuracy: 0.8045\n",
      "Epoch 3/20\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4377 - accuracy: 0.8291\n",
      "Epoch 4/20\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3809 - accuracy: 0.8398\n",
      "Epoch 5/20\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3310 - accuracy: 0.8545\n",
      "Epoch 6/20\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2990 - accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2726 - accuracy: 0.8692\n",
      "Epoch 8/20\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2585 - accuracy: 0.8655\n",
      "Epoch 9/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2424 - accuracy: 0.8780\n",
      "Epoch 10/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.8758\n",
      "Epoch 11/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2320 - accuracy: 0.8798\n",
      "Epoch 12/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2250 - accuracy: 0.8791\n",
      "Epoch 13/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2205 - accuracy: 0.8813\n",
      "Epoch 14/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2186 - accuracy: 0.8842\n",
      "Epoch 15/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2215 - accuracy: 0.8824\n",
      "Epoch 16/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.8824\n",
      "Epoch 17/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2108 - accuracy: 0.8857\n",
      "Epoch 18/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.8802\n",
      "Epoch 19/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2192 - accuracy: 0.8831\n",
      "Epoch 20/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbec684310>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(sentiment_tokenized, dummy_sentiment, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fa4c5",
   "metadata": {
    "id": "1b9fa4c5"
   },
   "source": [
    "# Test\n",
    "\n",
    "For testing, I use the same preprocessing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c954a3d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c954a3d4",
    "outputId": "81c092f1-7af8-490d-e00b-334d8b9b3537"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m parse_xml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLaptops_Test_p1_gold.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_text\u001b[49m(test_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_text' is not defined"
     ]
    }
   ],
   "source": [
    "test_df = parse_xml('Laptops_Test_p1_gold.xml')\n",
    "test_df = process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e0a7846",
   "metadata": {
    "id": "8e0a7846"
   },
   "outputs": [],
   "source": [
    "aspect_df = test_df.copy()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "aspect_terms = []\n",
    "for i, review in enumerate(nlp.pipe(aspect_df.processed)):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    if not len(chunks):\n",
    "        aspect_df.drop(i, inplace=True)\n",
    "        continue\n",
    "    aspect_terms.append(' '.join(chunks))\n",
    "\n",
    "aspect_df['aspect terms'] = aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b37b9038",
   "metadata": {
    "id": "b37b9038"
   },
   "outputs": [],
   "source": [
    "sentiment_df = test_df.copy()\n",
    "sentiment_terms = []\n",
    "rem_ind = []\n",
    "for i, review in enumerate(nlp.pipe(sentiment_df['processed'])):\n",
    "    chunks = [token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]\n",
    "    if not len(chunks):\n",
    "        rem_ind.append(i)\n",
    "        continue\n",
    "    sentiment_terms.append(' '.join(chunks))\n",
    "sentiment_df = sentiment_df.drop(rem_ind)\n",
    "sentiment_df['sentiment_terms'] = sentiment_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c81cefb4",
   "metadata": {
    "id": "c81cefb4"
   },
   "outputs": [],
   "source": [
    "vocab_size = 6000 \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(aspect_df['aspect terms']))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_entity = label_encoder.fit_transform(aspect_df.entity)\n",
    "entity_category = to_categorical(integer_entity)\n",
    "integer_attribute = label_encoder.fit_transform(aspect_df.attribute)\n",
    "attribute_category = to_categorical(integer_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc1fe88e",
   "metadata": {
    "id": "cc1fe88e"
   },
   "outputs": [],
   "source": [
    "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(sentiment_df.sentiment_terms))\n",
    "label_encoder = LabelEncoder()\n",
    "integer_sentiment = label_encoder.fit_transform(sentiment_df.polarity)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa8ac82f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa8ac82f",
    "outputId": "1339b5fb-1bc7-4974-9923-7cf83b7249e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 4ms/step - loss: 2.5160 - accuracy: 0.5428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5159709453582764, 0.5428156852722168]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_categories_model.evaluate(aspect_tokenized, entity_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa411077",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa411077",
    "outputId": "d6225ddf-a81b-47f8-f82c-7bfe03c61eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 2.6328 - accuracy: 0.2424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6327712535858154, 0.2423802614212036]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_categories_model.evaluate(aspect_tokenized, attribute_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe591cb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe591cb1",
    "outputId": "304f2e5b-45c2-416b-d6ea-da556ff71a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 1.8113 - accuracy: 0.5339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8112872838974, 0.5338753461837769]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.evaluate(sentiment_tokenized, dummy_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991731e",
   "metadata": {
    "id": "1991731e"
   },
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "# Part 2 \n",
    "\n",
    "Since splitting the text into separate sentences did not make much sense, I go for performing the preprocessing on the entire text as a whole. I follow the same methods as the first task except while evaluating sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87fdfb12",
   "metadata": {
    "id": "87fdfb12"
   },
   "outputs": [],
   "source": [
    "def parse_xml(filename):\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    data = ''.join(data)\n",
    "    content = bs(data, 'xml')\n",
    "    data = {\n",
    "        'text': [],\n",
    "        'entity': [],\n",
    "        'attribute': [],\n",
    "        'polarity': []\n",
    "    }\n",
    "\n",
    "    rid_list = []\n",
    "    a = list(content.find_all('Review'))\n",
    "    for i in a:\n",
    "        rid_list.append(i.get('rid'))\n",
    "        \n",
    "    for rid in rid_list:\n",
    "        sentences = list(content.find('Review', {'rid': rid}).find_all('sentence'))\n",
    "        opinions = list(content.find('Review', {'rid': rid}).find_all('Opinion'))\n",
    "        sentences = [x.text for x in sentences]\n",
    "        text = ' '.join(sentences)\n",
    "        if len(opinions):\n",
    "            for opinion in opinions:\n",
    "                data['text'].append(text)\n",
    "                category = opinion.get('category')\n",
    "                entity, attribute = category.split('#')\n",
    "                data['entity'].append(entity)\n",
    "                data['attribute'].append(attribute)\n",
    "                data['polarity'].append(opinion.get('polarity'))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "zH1wBZBjsflH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zH1wBZBjsflH",
    "outputId": "48b26188-120f-4c10-bed5-4024a7f17ad9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2082/2082 [00:00<00:00, 3152.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df = parse_xml('Laptops_Train_p2.xml')\n",
    "df = process_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "kjvf-aPS1WPu",
   "metadata": {
    "id": "kjvf-aPS1WPu"
   },
   "outputs": [],
   "source": [
    "entity_labels = ['laptop', 'display', 'keyboard', 'mouse', 'motherboard', 'cpu', 'fans_cooling', \n",
    "                 'ports', 'memory', 'power_supply', 'optical_drives', 'battery', 'graphics', \n",
    "                 'hard_disk', 'multimedia_devices', 'hardware', 'software', 'os', 'warranty', 'shipping', 'support', 'company']\n",
    "\n",
    "attribute_labels = ['general', 'price', 'quality', 'design_feature', 'operation_performance', 'usability', 'portability',\n",
    "                   'connectivity', 'miscellaneous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "w8RT7sxM1lbh",
   "metadata": {
    "id": "w8RT7sxM1lbh"
   },
   "outputs": [],
   "source": [
    "df = orig.copy()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "aspect_terms = []\n",
    "for i, review in enumerate(nlp.pipe(df.processed)):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    if not len(chunks):\n",
    "        df.drop(i, inplace=True)\n",
    "        continue\n",
    "    aspect_terms.append(' '.join(chunks))\n",
    "\n",
    "df['aspect terms'] = aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "n1BXU91615-t",
   "metadata": {
    "id": "n1BXU91615-t"
   },
   "outputs": [],
   "source": [
    "entity_categories_model = Sequential()\n",
    "entity_categories_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "entity_categories_model.add(Dense(512, activation='relu'))\n",
    "entity_categories_model.add(Dense(128, activation='relu'))\n",
    "entity_categories_model.add(Dense(len(entity_labels), activation='softmax'))\n",
    "entity_categories_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8BiCj4zM1-A_",
   "metadata": {
    "id": "8BiCj4zM1-A_"
   },
   "outputs": [],
   "source": [
    "vocab_size = 6000 \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(df['aspect terms']))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_entity = label_encoder.fit_transform(df.entity)\n",
    "entity_category = to_categorical(integer_entity)\n",
    "integer_attribute = label_encoder.fit_transform(df.attribute)\n",
    "attribute_category = to_categorical(integer_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "Vru1rrR51_KA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vru1rrR51_KA",
    "outputId": "513a35f6-b65b-45ae-b09a-5e724f8f62cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "78/78 [==============================] - 2s 5ms/step - loss: 1.7477 - accuracy: 0.6308\n",
      "Epoch 2/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.3332 - accuracy: 0.6482\n",
      "Epoch 3/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0727 - accuracy: 0.6891\n",
      "Epoch 4/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8905 - accuracy: 0.7417\n",
      "Epoch 5/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.7893 - accuracy: 0.7510\n",
      "Epoch 6/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.7243 - accuracy: 0.7575\n",
      "Epoch 7/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.7055 - accuracy: 0.7660\n",
      "Epoch 8/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.7676\n",
      "Epoch 9/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6350 - accuracy: 0.7830\n",
      "Epoch 10/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.6198 - accuracy: 0.7741\n",
      "Epoch 11/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5979 - accuracy: 0.7814\n",
      "Epoch 12/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5843 - accuracy: 0.7769\n",
      "Epoch 13/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5741 - accuracy: 0.7769\n",
      "Epoch 14/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5669 - accuracy: 0.7838\n",
      "Epoch 15/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5604 - accuracy: 0.7814\n",
      "Epoch 16/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5571 - accuracy: 0.7838\n",
      "Epoch 17/30\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5418 - accuracy: 0.7822\n",
      "Epoch 18/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5400 - accuracy: 0.7806\n",
      "Epoch 19/30\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5389 - accuracy: 0.7834\n",
      "Epoch 20/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5349 - accuracy: 0.7753\n",
      "Epoch 21/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5404 - accuracy: 0.7814\n",
      "Epoch 22/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5266 - accuracy: 0.7810\n",
      "Epoch 23/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5230 - accuracy: 0.7866\n",
      "Epoch 24/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5241 - accuracy: 0.7798\n",
      "Epoch 25/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5258 - accuracy: 0.7891\n",
      "Epoch 26/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5180 - accuracy: 0.7858\n",
      "Epoch 27/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.7789\n",
      "Epoch 28/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5087 - accuracy: 0.7838\n",
      "Epoch 29/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5094 - accuracy: 0.7838\n",
      "Epoch 30/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.5117 - accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbc9ecd1f0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_categories_model.fit(aspect_tokenized, entity_category, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "GzSWtJvg2BDL",
   "metadata": {
    "id": "GzSWtJvg2BDL"
   },
   "outputs": [],
   "source": [
    "attribute_categories_model = Sequential()\n",
    "attribute_categories_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "attribute_categories_model.add(Dense(512, activation='relu'))\n",
    "attribute_categories_model.add(Dense(128, activation='relu'))\n",
    "attribute_categories_model.add(Dense(len(attribute_labels), activation='softmax'))\n",
    "attribute_categories_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "F1CwxVwP2E9i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1CwxVwP2E9i",
    "outputId": "b6ba7be7-1e5d-453f-bdbe-0104cf9a2694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "78/78 [==============================] - 2s 5ms/step - loss: 1.8983 - accuracy: 0.3089\n",
      "Epoch 2/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.5994 - accuracy: 0.4462\n",
      "Epoch 3/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.3780 - accuracy: 0.5121\n",
      "Epoch 4/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.2578 - accuracy: 0.5401\n",
      "Epoch 5/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.1748 - accuracy: 0.5478\n",
      "Epoch 6/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.1180 - accuracy: 0.5575\n",
      "Epoch 7/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0864 - accuracy: 0.5547\n",
      "Epoch 8/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0545 - accuracy: 0.5547\n",
      "Epoch 9/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0270 - accuracy: 0.5587\n",
      "Epoch 10/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 1.0114 - accuracy: 0.5668\n",
      "Epoch 11/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9882 - accuracy: 0.5595\n",
      "Epoch 12/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9827 - accuracy: 0.5757\n",
      "Epoch 13/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9644 - accuracy: 0.5802\n",
      "Epoch 14/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9497 - accuracy: 0.5676\n",
      "Epoch 15/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9353 - accuracy: 0.5854\n",
      "Epoch 16/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9275 - accuracy: 0.5757\n",
      "Epoch 17/30\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.9264 - accuracy: 0.5846\n",
      "Epoch 18/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9128 - accuracy: 0.5798\n",
      "Epoch 19/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9145 - accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9146 - accuracy: 0.5794\n",
      "Epoch 21/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.9053 - accuracy: 0.5789\n",
      "Epoch 22/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.9018 - accuracy: 0.5866\n",
      "Epoch 23/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8968 - accuracy: 0.5830\n",
      "Epoch 24/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8875 - accuracy: 0.6008\n",
      "Epoch 25/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.8858 - accuracy: 0.5879\n",
      "Epoch 26/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8846 - accuracy: 0.5931\n",
      "Epoch 27/30\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.8769 - accuracy: 0.5919\n",
      "Epoch 28/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8776 - accuracy: 0.5862\n",
      "Epoch 29/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8746 - accuracy: 0.5866\n",
      "Epoch 30/30\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.8735 - accuracy: 0.5866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbd4a5b610>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_categories_model.fit(aspect_tokenized, attribute_category, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bfw1FaCH2Gop",
   "metadata": {
    "id": "bfw1FaCH2Gop"
   },
   "outputs": [],
   "source": [
    "df = orig.copy()\n",
    "sentiment_terms = []\n",
    "rem_ind = []\n",
    "for i, review in enumerate(nlp.pipe(df['processed'])):\n",
    "    chunks = [token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]\n",
    "    if not len(chunks):\n",
    "        rem_ind.append(i)\n",
    "        continue\n",
    "    sentiment_terms.append(' '.join(chunks))\n",
    "df = df.drop(rem_ind)\n",
    "df['sentiment_terms'] = sentiment_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2cMoDMFp2I9P",
   "metadata": {
    "id": "2cMoDMFp2I9P"
   },
   "outputs": [],
   "source": [
    "sentiment_model = Sequential()\n",
    "sentiment_model.add(Dense(1024, input_shape=(6000,), activation='relu'))\n",
    "sentiment_model.add(Dense(512, activation='relu'))\n",
    "sentiment_model.add(Dense(256, activation='relu'))\n",
    "sentiment_model.add(Dense(128, activation='relu'))\n",
    "sentiment_model.add(Dense(3, activation='softmax'))\n",
    "sentiment_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "KT4ZsLy62KJI",
   "metadata": {
    "id": "KT4ZsLy62KJI"
   },
   "outputs": [],
   "source": [
    "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(df.sentiment_terms))\n",
    "label_encoder = LabelEncoder()\n",
    "integer_sentiment = label_encoder.fit_transform(df.polarity)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "-BPw2sRW2L9R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BPw2sRW2L9R",
    "outputId": "b9b0b6c3-0c65-457e-dab3-1f2af53bcc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "86/86 [==============================] - 3s 5ms/step - loss: 0.7458 - accuracy: 0.6935\n",
      "Epoch 2/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5298 - accuracy: 0.7931\n",
      "Epoch 3/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8269\n",
      "Epoch 4/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.3613 - accuracy: 0.8409\n",
      "Epoch 5/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8556\n",
      "Epoch 6/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2885 - accuracy: 0.8692\n",
      "Epoch 7/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.8640\n",
      "Epoch 8/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2554 - accuracy: 0.8728\n",
      "Epoch 9/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2524 - accuracy: 0.8750\n",
      "Epoch 10/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2364 - accuracy: 0.8773\n",
      "Epoch 11/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2259 - accuracy: 0.8798\n",
      "Epoch 12/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2216 - accuracy: 0.8795\n",
      "Epoch 13/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.8791\n",
      "Epoch 14/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2170 - accuracy: 0.8846\n",
      "Epoch 15/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2166 - accuracy: 0.8795\n",
      "Epoch 16/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2218 - accuracy: 0.8853\n",
      "Epoch 17/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2199 - accuracy: 0.8853\n",
      "Epoch 18/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.8754\n",
      "Epoch 19/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2167 - accuracy: 0.8820\n",
      "Epoch 20/20\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.8890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbec4db100>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(sentiment_tokenized, dummy_sentiment, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k-OIGp9U2ZOr",
   "metadata": {
    "id": "k-OIGp9U2ZOr"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "czfes8Ms2M8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czfes8Ms2M8d",
    "outputId": "f3817c95-3d69-401a-afc6-e978e5021272"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 545/545 [00:00<00:00, 959.56it/s] \n"
     ]
    }
   ],
   "source": [
    "test_df = parse_xml('Laptops_Test_p2_gold.xml')\n",
    "test_df = process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9JKCF7Q2Sto",
   "metadata": {
    "id": "e9JKCF7Q2Sto"
   },
   "outputs": [],
   "source": [
    "aspect_df = test_df.copy()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "aspect_terms = []\n",
    "for i, review in enumerate(nlp.pipe(aspect_df.processed)):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    if not len(chunks):\n",
    "        aspect_df.drop(i, inplace=True)\n",
    "        continue\n",
    "    aspect_terms.append(' '.join(chunks))\n",
    "\n",
    "aspect_df['aspect terms'] = aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "-qNzD4vl2co9",
   "metadata": {
    "id": "-qNzD4vl2co9"
   },
   "outputs": [],
   "source": [
    "sentiment_df = test_df.copy()\n",
    "sentiment_terms = []\n",
    "rem_ind = []\n",
    "for i, review in enumerate(nlp.pipe(sentiment_df['processed'])):\n",
    "    chunks = [token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]\n",
    "    if not len(chunks):\n",
    "        rem_ind.append(i)\n",
    "        continue\n",
    "    sentiment_terms.append(' '.join(chunks))\n",
    "sentiment_df = sentiment_df.drop(rem_ind)\n",
    "sentiment_df['sentiment_terms'] = sentiment_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "H2EmdHsc2dnl",
   "metadata": {
    "id": "H2EmdHsc2dnl"
   },
   "outputs": [],
   "source": [
    "vocab_size = 6000 \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(aspect_df['aspect terms']))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_entity = label_encoder.fit_transform(aspect_df.entity)\n",
    "entity_category = to_categorical(integer_entity)\n",
    "integer_attribute = label_encoder.fit_transform(aspect_df.attribute)\n",
    "attribute_category = to_categorical(integer_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "m7yLjJTZ2ehW",
   "metadata": {
    "id": "m7yLjJTZ2ehW"
   },
   "outputs": [],
   "source": [
    "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(sentiment_df.sentiment_terms))\n",
    "label_encoder = LabelEncoder()\n",
    "integer_sentiment = label_encoder.fit_transform(sentiment_df.polarity)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "w7G1FZUY2fUM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7G1FZUY2fUM",
    "outputId": "8d48e579-0c10-4d50-8ea6-83936f684015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4855 - accuracy: 0.4697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.485455513000488, 0.46972477436065674]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_categories_model.evaluate(aspect_tokenized, entity_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "mosBIc9T2gU6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mosBIc9T2gU6",
    "outputId": "1761ed52-7484-4a38-be2a-5f2493805b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4855 - accuracy: 0.4697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.485455513000488, 0.46972477436065674]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_categories_model.evaluate(aspect_tokenized, entity_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f7f13",
   "metadata": {},
   "source": [
    "Here, I check if the maximum value of the prediction vector is greater than 0.5 i.e. the model is confident about its predictions. If it is, I go ahead with it (I use the inverse encoder to get the original labels back). If not, I label it as 'conflict'. I then proceed to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "Iv-bKJay2htR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv-bKJay2htR",
    "outputId": "eee0138b-ce40-4f2f-8ca6-4c43f7a3ca5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y = sentiment_model.predict(sentiment_tokenized)\n",
    "preds = []\n",
    "for i in range(len(y)):\n",
    "    m = max(y[i])\n",
    "    if m<0.5:\n",
    "        preds.append('conflict')\n",
    "    else:\n",
    "        a = np.argmax(y[i])\n",
    "        preds.append(str(label_encoder.inverse_transform([a])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "87Q73Hx96m7K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87Q73Hx96m7K",
    "outputId": "897a7a3d-93ec-4f60-97ac-349ca5ea42c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8710033076074972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(list(df.polarity), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad87eba",
   "metadata": {
    "id": "kMUOImo48pQ2"
   },
   "source": [
    "# Final thoughts\n",
    "\n",
    "This can of course be improved by a lot. For now, this model can only predict one pair of aspect-sentiment for each query, as I was not able to build a model that can output multiple pairs for the same query. using a pre-trained model would also yield much better accuracies as well. As for the preprocessing part, a better spell-checker could have been used. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
